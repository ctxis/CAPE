'''
Test case support for DC3-MWCP. Parser output is stored in a json file per parser. To run test cases,
parser is re-run and compared to previous results.

'''
# Standard imports
import os
import json
import glob

# MWCP framework imports
#from mwcp.malwareconfigreporter import malwareconfigreporter

DEFAULT_EXCLUDE_FIELDS = ["debug"]

class malwareconfigtester(object):
    '''
    DC3-MWCP test case class
    '''
    # Constants
    INPUT_FILE_PATH = "inputfilename"
    FILE_EXTENSION = ".json"
    PARSER = "parser"
    RESULTS = "results"
    PASSED = "passed"
    ERRORS = "errors"

    # Properties
    reporter = None
    results_dir = None
    
    def __init__(self, reporter, results_dir):
        self.reporter = reporter
        self.results_dir = results_dir

    def gen_results(self, parser_name, input_file_path):
        '''
        Generate JSON results for the given file using the given parser name.
        '''

        self.reporter.run_parser(parser_name, input_file_path)
        self.reporter.metadata[self.INPUT_FILE_PATH] = input_file_path
        
        for error in self.reporter.errors:
            print(error)
            
        return self.reporter.metadata     

    def list_test_files(self, parser_name):
        '''
        Generate list of files (test cases) for parser
        '''
        filelist = []
        for metadata in self.parse_results_file(self.get_results_filepath(parser_name)):
            filelist.append(metadata[self.INPUT_FILE_PATH])
        return filelist
    
    def get_results_filepath(self, parser_name):
        '''
        Get a results file path based on the parser name provided and the
        previously specified output directory.
        '''

        file_name = parser_name + self.FILE_EXTENSION
        file_path = os.path.join(self.results_dir, file_name)
        
        return file_path

    def parse_results_file(self,
                           results_file_path):
        '''
        Parse the the JSON results file and return the parsed data.
        '''

        with open(results_file_path) as results_file:
            data = json.load(results_file)

        # The results file data is expected to be a list of metadata dictionaries
        assert type(data) == list and all(type(a) is dict for a in data)
        
        return data


    def update_test_results(self,
                            results_file_path,
                            results_data,
                            replace = True):
        '''
        Update results in the results file with the passed in results data. If the
        file path for the results data matches a file path that is already found in
        the passed in results file, then the replace argument comes into play to
        determine if the record should be replaced.
        '''

        # The results data is expected to be a dictionary representing results for a single file
        assert type(results_data) is dict

        results_file_data = []
        if os.path.isfile(results_file_path):            
            results_file_data = self.parse_results_file(results_file_path)
            
            # Check if there is a duplicate file path already in the results path
            index = 0
            found = False
            while index < len(results_file_data) and not found:
                metadata = results_file_data[index]
                if metadata[self.INPUT_FILE_PATH] == results_data[self.INPUT_FILE_PATH]:
                    if replace:
                        results_file_data[index] = results_data     
                    found = True
                index += 1

            # If no duplicate found, then append the passed in results data to existing results
            if not found:
                results_file_data.append(results_data)
                                                                       
        else:
            # Results file should be a list of metadata dictionaries
            results_file_data.append(results_data)
            
        # Write updated data to results file
        pretty_data = self.reporter.pprint(results_file_data)
        with open(results_file_path, "w") as results_file:
            results_file.write(pretty_data)

            
    def remove_test_results(self, parser_name, filenames):
        '''
        remove filenames from test cases for parser_name
        
        return files that were removed
        '''
        removed_files = []
        results_file_data = []
        for metadata in self.parse_results_file(self.get_results_filepath(parser_name)):
            if metadata[self.INPUT_FILE_PATH] in filenames:
                removed_files.append(metadata[self.INPUT_FILE_PATH])
            else:
                results_file_data.append(metadata)
        
        pretty_data = self.reporter.pprint(results_file_data)
        with open(self.get_results_filepath(parser_name), "w") as results_file:
            results_file.write(pretty_data)
        
        return removed_files
        
    def run_tests(self, parser_names = None, field_names = None, ignore_field_names = DEFAULT_EXCLUDE_FIELDS):
        '''

        Run tests and compare produced results to expected results.

        Arguments:
            parser_names (list):
                A list of parser names to run tests for. If the list is empty (default),
                then test cases for all parsers will be run.
            field_names(list):
                A restricted list of fields (metadata key values) that should be compared
                during testing. If the list is empty (default), then all fields, except those in
                ignore_field_names will be compared.
        '''

        results_file_list = glob.glob(os.path.join(self.results_dir, "*{0}".format(self.FILE_EXTENSION)))
        all_test_results = []
        all_passed = True
        if not parser_names:
            parser_names = []
        if not field_names:
            field_names = []

        # Determine files to test (this will be a list of JSON files)
        test_case_file_paths = []
        if len(parser_names) > 0:
            for parser_name in parser_names:
                results_file_path = self.get_results_filepath(parser_name)
                
                if results_file_path in results_file_list:
                    test_case_file_paths.append(results_file_path)                                                                     
                else:
                    print "Results file not found for {0} parser".format(parser_name)
                    print "File not found = {0}".format(results_file_path)
        else:
            test_case_file_paths = results_file_list

        # Parse test case/results files, run tests, and compare expected results to produced results
        for results_file_path in test_case_file_paths:
            results_data = self.parse_results_file(results_file_path)
            parser_name = os.path.splitext(os.path.basename(results_file_path))[0]

            for result_data in results_data:
                input_file_path = result_data[self.INPUT_FILE_PATH]
                new_results = self.gen_results(parser_name, input_file_path)
                passed, test_results = self.compare_results(result_data, new_results, field_names, ignore_field_names=ignore_field_names)
                errors = False
                if len(self.reporter.errors) > 0:
                    passed = False
                    errors = True
                all_test_results.append({self.PARSER: parser_name,
                                         self.INPUT_FILE_PATH: input_file_path,
                                         self.PASSED: passed,
                                         self.ERRORS: errors,
                                         self.RESULTS: test_results})
                if not passed:
                    all_passed = False

        # Return tuple showing if any tests failed alongside more detailed results
        return all_passed, all_test_results

    def compare_results(self, results_a, results_b, field_names = [], ignore_field_names = DEFAULT_EXCLUDE_FIELDS):
        '''
        Compare two result sets. If the field names list is not empty,
        then only the fields (metadata key values) in the list will be compared. ignore_field_names fields are not compared unless included in field_names.
        '''

        passed = True
        test_results = {}
        if not field_names:
            field_names = []

        # Cursory check to remove FILE_INPUT_PATH key from results since it is a custom added field for test cases
        if self.INPUT_FILE_PATH in results_a:
            results_a = dict(results_a)
            del results_a[self.INPUT_FILE_PATH]
        if self.INPUT_FILE_PATH in results_b:
            results_b = dict(results_b)
            del results_b[self.INPUT_FILE_PATH]

        # Begin comparing results
        if len(field_names) > 0:
            for field_name in field_names:
                # Ensure field name is in both result sets
                if field_name in results_a and field_name in results_b:
                    compare_result = self.compare_results_field(results_a, results_b, field_name)
                    test_results[field_name] = compare_result
                    if compare_result == False:
                        passed = False
                # Field in one result set but not the other
                elif field_name in results_a or field_name in results_b:
                    test_results[field_name] = False
                    passed = False
        else:
            for ignore_field in ignore_field_names:
                results_a.pop(ignore_field,None)
                results_b.pop(ignore_field,None)
            if set(results_a.keys()) != set(results_b.keys()):           
                passed = False
                all_keys = set(results_a.keys()).union(results_b.keys())
                for key in all_keys:
                    test_results[key] = self.compare_results_field(results_a, results_b, key)
            else:
                for key in results_a:
                    compare_result = self.compare_results_field(results_a, results_b, key)
                    test_results[key] = compare_result
                    if compare_result == False:
                        passed = False

        return passed, test_results


    def compare_results_field(self, results_a, results_b, field_name):
        '''
        Compare the values for a single results field in the two passed in results.
        '''

        assert type(results_a) is dict and type(results_b) is dict

        # Check if provided field_name is a valid key (based on fields.json)
        try:
            field_name_u = self.reporter.convert_to_unicode(field_name)
        except Exception as e:
            print "Error comparing metadata due to failure converting key to unicode: %s" % (str(e))
            return False
        
        if field_name_u in self.reporter.fields:
            field_type = self.reporter.fields[field_name_u]['type']
        else:
            print "Error comparing metadata because %s is not an allowed key" % field_name_u
            return False

        # Confirm key is found in the passed in result sets
        if field_name_u not in results_a or field_name_u not in results_b:
            return False

        # Now compare results based on field type (see "fields.json" for more details)
        if field_type == "listofstrings":
            return set(results_a[field_name_u]) == set(results_b[field_name_u])
        elif field_type == "listofstringtuples":
            set_list_a = [set(x) for x in results_a[field_name_u]]
            set_list_b = [set(x) for x in results_b[field_name_u]]
            
            if len(set_list_a) != len(set_list_b):
                return False
            
            for set_a in set_list_a:
                if set_a not in set_list_b:
                    return False
                
            return True
        elif field_type == "dictofstrings":
            dict_a = results_a[field_name_u]
            dict_b = results_b[field_name_u]

            if set(dict_a.keys()) != set(dict_b.keys()):
                return False
            
            for key in dict_a:
                if dict_a[key] != dict_b[key]:
                    return False
                
            return True

        # This point shouldn't be reached unless the "fields.json" file has added field types added
        return False

    def print_test_results(self, test_results, failed_tests = True, passed_tests = True, verbose = False, json_format = False):
        '''
        Print test results based on provided parameters. Expects results format
        produced by run_tests() function.
        '''

        if json_format:
            filtered_output = []
            for test_result in test_results:
                passed = test_result[self.PASSED]
                if (passed and passed_tests) or (not passed and failed_tests):
                    if verbose:
                        filtered_output.append(test_result)
                    else:
                        filtered_result = {self.PARSER : test_result[self.PARSER],
                                           self.INPUT_FILE_PATH: test_result[self.INPUT_FILE_PATH],
                                           self.PASSED: test_result[self.PASSED]}
                        filtered_output.append(filtered_result)
                        
            print json.dumps(filtered_output, indent = 4)
        else:
            separator = ""
            
            for test_result in test_results:
                filtered_output = ""
                passed = test_result[self.PASSED]
                if (passed and passed_tests) or (not passed and failed_tests):
                    filtered_output += "{0} = {1}\n".format(self.PARSER, test_result[self.PARSER])
                    filtered_output += "{0} = {1}\n".format(self.INPUT_FILE_PATH, test_result[self.INPUT_FILE_PATH])
                    filtered_output += "{0} = {1}\n".format(self.PASSED, test_result[self.PASSED])
                    filtered_output += "{0} = {1}\n".format(self.ERRORS, test_result[self.ERRORS])

                    if verbose:
                        filtered_output += "{0}:\n".format(self.RESULTS)
                        for key in test_result[self.RESULTS]:
                            filtered_output += "\t{0} = {1}\n".format(key, test_result[self.RESULTS][key])
                if filtered_output != "":
                    filtered_output += "{0}\n".format(separator)
                    print filtered_output
                    
        
            










